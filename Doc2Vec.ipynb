{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "# numpy\n",
    "import numpy\n",
    "# random\n",
    "from random import shuffle\n",
    "# classifier\n",
    "from sklearn import linear_model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/home/owner/デスクトップ/PythonFile/imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numEpochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'data/tr_pos.txt':'TRAIN_POS_', 'data/tr_neg.txt':'TRAIN_NEG_', 'data/te_pos.txt':'TEST_POS_', 'data/te_neg.txt':'TEST_NEG_'}\n",
    "#{'tr_data/tr_pos.txt':'TRAIN_POS_', 'tr_data/tr_neg.txt':'TRAIN_NEG_', 'te_data/te_pos.txt':'TEST_POS_', 'te_data/te_neg.txt':'TEST_NEG_'}\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/home/owner/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train(sentences.sentences_perm(), total_examples=model.corpus_count, epochs=numEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('./imdb_'+str(numEpochs)+'Epochs.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('./imdb_'+str(numEpochs)+'Epochs.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('eat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((25000, 100))\n",
    "train_labels = numpy.zeros(25000)\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS__' + str(i)\n",
    "    prefix_train_neg ='TRAIN_NEG__' + str(i)\n",
    "    train_arrays[i] = model[prefix_train_pos]\n",
    "    train_arrays[12500 + i] = model[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((25000, 100))\n",
    "test_labels = numpy.zeros(25000)\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS__' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG__' + str(i)\n",
    "    test_arrays[i] = model[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_logistic = linear_model.LogisticRegression()\n",
    "classifier_logistic.fit(train_arrays, train_labels)\n",
    "classifier_logistic.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_lasso = linear_model.Lasso()\n",
    "classifier_lasso.fit(train_arrays, train_labels)\n",
    "classifier_lasso.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_OLS = linear_model.LinearRegression()\n",
    "classifier_OLS.fit(train_arrays, train_labels)\n",
    "classifier_OLS.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_ridge = linear_model.Ridge()\n",
    "classifier_ridge.fit(train_arrays, train_labels)\n",
    "classifier_ridge.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_elastic = linear_model.ElasticNet()\n",
    "classifier_elastic.fit(train_arrays, train_labels)\n",
    "classifier_elastic.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_LARS = linear_model.Lars()\n",
    "classifier_LARS.fit(train_arrays, train_labels)\n",
    "classifier_LARS.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_OMP = linear_model.OrthogonalMatchingPursuit(n_nonzero_coefs=100)\n",
    "classifier_OMP.fit(train_arrays, train_labels)\n",
    "classifier_OMP.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_lassoLARS = linear_model.LassoLars(alpha=.1)\n",
    "classifier_lassoLARS.fit(train_arrays, train_labels)\n",
    "classifier_lassoLARS.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_BayesianRidge = linear_model.BayesianRidge()\n",
    "classifier_BayesianRidge.fit(train_arrays, train_labels)\n",
    "classifier_BayesianRidge.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = [ 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "best = 0\n",
    "for loss in losses:\n",
    "    classifier_SGD = linear_model.SGDClassifier(loss=loss)\n",
    "    classifier_SGD.fit(train_arrays, train_labels)\n",
    "    score = classifier_SGD.score(test_arrays, test_labels)\n",
    "    if (score>best):\n",
    "        best = score\n",
    "        best_loss = loss\n",
    "        \n",
    "print(best_loss, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_Perceptron = linear_model.Perceptron()\n",
    "classifier_Perceptron.fit(train_arrays, train_labels)\n",
    "classifier_Perceptron.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_PAC = linear_model.PassiveAggressiveClassifier(loss='hinge')\n",
    "classifier_PAC.fit(train_arrays, train_labels)\n",
    "print(classifier_PAC.score(test_arrays, test_labels))\n",
    "\n",
    "classifier_PAC2 = linear_model.PassiveAggressiveClassifier(loss='squared_hinge')\n",
    "classifier_PAC2.fit(train_arrays, train_labels)\n",
    "print(classifier_PAC2.score(test_arrays, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, merge\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda, regularizers, Average\n",
    "from keras.layers import Input, Conv2D, Conv1D, MaxPooling2D, GlobalMaxPooling2D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.core import Dropout, Dense, Lambda, Masking\n",
    "from keras.layers import merge, Layer, Activation, Dot, Concatenate, Flatten, Lambda\n",
    "from keras.initializers import Identity,glorot_normal\n",
    "from keras import regularizers\n",
    "from keras import metrics\n",
    "from keras.utils import plot_model\n",
    "import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numOfDocs = len(train_arrays)\n",
    "sizeOfVectors = len(train_arrays[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_simple_NN = Sequential()\n",
    "#classifier_simple_NN.add(Input( shape = ( sizeOfVectors, 1 ) , name='Input' ))\n",
    "classifier_simple_NN.add(Dense(sizeOfVectors*2, input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(0.01)))\n",
    "classifier_simple_NN.add(Dropout(0.5))\n",
    "classifier_simple_NN.add(Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "classifier_simple_NN.compile(loss='binary_crossentropy',\n",
    "                             optimizer = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                            metrics=['accuracy'])\n",
    "classifier_simple_NN.fit(train_arrays, train_labels, batch_size=20, epochs=80,validation_data=(test_arrays,test_labels), shuffle=True)\n",
    "score = classifier_simple_NN.evaluate(test_arrays, test_labels)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN1D 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_CNN = Sequential()\n",
    "classifier_CNN.add(Lambda(lambda x: K.expand_dims(x), input_shape=(sizeOfVectors,)))\n",
    "# classifier_CNN.add(Lambda(lambda x: K.squeeze(x, 2)))\n",
    "#classifier_CNN.add(Input( shape = ( sizeOfVectors, 1 ) , name='Input' ))\n",
    "classifier_CNN.add(Conv1D(10, kernel_size = 30, padding='valid',input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(1e-5)))\n",
    "classifier_CNN.add(Dropout(0.5))\n",
    "classifier_CNN.add(Conv1D(10, kernel_size = 30, padding='valid',input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(1e-5)))\n",
    "classifier_CNN.add(Dropout(0.5))\n",
    "classifier_CNN.add(Conv1D(10, kernel_size = 30, padding='valid',input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(1e-5)))\n",
    "classifier_CNN.add(Dropout(0.5))\n",
    "classifier_CNN.add(Flatten())\n",
    "classifier_CNN.add(Dense(75, activation='tanh',kernel_regularizer=regularizers.l2(1e-5)))\n",
    "classifier_CNN.add(Dropout(0.5))\n",
    "classifier_CNN.add(Dense(100, activation='tanh',kernel_regularizer=regularizers.l2(1e-5)))\n",
    "classifier_CNN.add(Dropout(0.5))\n",
    "classifier_CNN.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "classifier_CNN.compile(loss='binary_crossentropy',\n",
    "                       optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "classifier_CNN.fit(train_arrays, train_labels, batch_size=50, epochs=10,validation_data=(test_arrays,test_labels), shuffle=True)\n",
    "score = classifier_CNN.evaluate(test_arrays, test_labels)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = classifier_CNN.evaluate(test_arrays, test_labels)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_CNN.fit(train_arrays, train_labels, batch_size=200,initial_epoch=10, epochs=15, validation_data=(test_arrays,test_labels), shuffle=True)\n",
    "score = classifier_CNN.evaluate(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = classifier_CNN.evaluate(test_arrays, test_labels)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_LSTM = Sequential()\n",
    "classifier_LSTM.add(Lambda(lambda x: K.expand_dims(x), input_shape=(sizeOfVectors,)))\n",
    "# classifier_CNN.add(Lambda(lambda x: K.squeeze(x, 2)))\n",
    "#classifier_CNN.add(Input( shape = ( sizeOfVectors, 1 ) , name='Input' ))\n",
    "classifier_LSTM.add(LSTM(10,input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(1e-5),\n",
    "                       recurrent_regularizer=regularizers.l2(1e-5),bias_regularizer=regularizers.l2(1e-5),\n",
    "                       dropout=0.3, recurrent_dropout=0.3))\n",
    "classifier_LSTM.add(Dropout(0.5))\n",
    "classifier_LSTM.add(Dense(100, activation='tanh',kernel_regularizer=regularizers.l2(1e-5)))\n",
    "classifier_LSTM.add(Dropout(0.5))\n",
    "classifier_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "classifier_LSTM.compile(loss='binary_crossentropy',\n",
    "                       optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "classifier_LSTM.fit(train_arrays, train_labels, batch_size=100, epochs=100, validation_data=(test_arrays,test_labels), shuffle=True)\n",
    "score = classifier_RNN.evaluate(test_arrays, test_labels)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_3LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 3 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_3LSTM = Sequential()\n",
    "classifier_3LSTM.add(Lambda(lambda x: K.expand_dims(x), input_shape=(sizeOfVectors,)))\n",
    "# classifier_CNN.add(Lambda(lambda x: K.squeeze(x, 2)))\n",
    "#classifier_CNN.add(Input( shape = ( sizeOfVectors, 1 ) , name='Input' ))\n",
    "classifier_3LSTM.add(LSTM(10,input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(1e-5),\n",
    "                       recurrent_regularizer=regularizers.l2(1e-5),bias_regularizer=regularizers.l2(1e-5),\n",
    "                       dropout=0.3, recurrent_dropout=0.3))\n",
    "classifier_3LSTM.add(Lambda(lambda x: K.expand_dims(x)))\n",
    "classifier_3LSTM.add(LSTM(5,input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(1e-5),\n",
    "                       recurrent_regularizer=regularizers.l2(1e-5),bias_regularizer=regularizers.l2(1e-5),\n",
    "                       dropout=0.3, recurrent_dropout=0.3))\n",
    "classifier_3LSTM.add(Lambda(lambda x: K.expand_dims(x)))\n",
    "classifier_3LSTM.add(LSTM(10,input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(1e-5),\n",
    "                       recurrent_regularizer=regularizers.l2(1e-5),bias_regularizer=regularizers.l2(1e-5),\n",
    "                       dropout=0.3, recurrent_dropout=0.3))\n",
    "classifier_3LSTM.add(Dropout(0.5))\n",
    "classifier_3LSTM.add(Dense(100, activation='tanh',kernel_regularizer=regularizers.l2(1e-5)))\n",
    "classifier_3LSTM.add(Dropout(0.5))\n",
    "classifier_3LSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "classifier_3LSTM.compile(loss='binary_crossentropy',\n",
    "                       optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "classifier_3LSTM.fit(train_arrays, train_labels, batch_size=100, epochs=120, validation_data=(test_arrays,test_labels), shuffle=True)\n",
    "score = classifier_RNN.evaluate(test_arrays, test_labels)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BILSTM 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_BILSTM = Sequential()\n",
    "classifier_BILSTM.add(Lambda(lambda x: K.expand_dims(x), input_shape=(sizeOfVectors,)))\n",
    "# classifier_CNN.add(Lambda(lambda x: K.squeeze(x, 2)))\n",
    "#classifier_CNN.add(Input( shape = ( sizeOfVectors, 1 ) , name='Input' ))\n",
    "classifier_BILSTM.add(Bidirectional(LSTM(10,input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(1e-5),\n",
    "                       recurrent_regularizer=regularizers.l2(1e-5),bias_regularizer=regularizers.l2(1e-5),\n",
    "                       dropout=0.3, recurrent_dropout=0.3), merge_mode=\"concat\"))\n",
    "classifier_BILSTM.add(Dropout(0.5))\n",
    "classifier_BILSTM.add(Dense(100, activation='tanh',kernel_regularizer=regularizers.l2(1e-5)))\n",
    "classifier_BILSTM.add(Dropout(0.5))\n",
    "classifier_BILSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "classifier_BILSTM.compile(loss='binary_crossentropy',\n",
    "                       optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "classifier_BILSTM.fit(train_arrays, train_labels, batch_size=100, epochs=100, validation_data=(test_arrays,test_labels), shuffle=True)\n",
    "score = classifier_RNN.evaluate(test_arrays, test_labels)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_BILSTM = Sequential()\n",
    "classifier_BILSTM.add(Lambda(lambda x: K.expand_dims(x), input_shape=(sizeOfVectors,)))\n",
    "# classifier_CNN.add(Lambda(lambda x: K.squeeze(x, 2)))\n",
    "#classifier_CNN.add(Input( shape = ( sizeOfVectors, 1 ) , name='Input' ))\n",
    "classifier_BILSTM.add(Bidirectional(LSTM(10,input_dim=sizeOfVectors,kernel_regularizer=regularizers.l2(1e-5),\n",
    "                       recurrent_regularizer=regularizers.l2(1e-5),bias_regularizer=regularizers.l2(1e-5),\n",
    "                       dropout=0.3, recurrent_dropout=0.3), merge_mode=\"concat\"))\n",
    "classifier_BILSTM.add(Dropout(0.5))\n",
    "classifier_BILSTM.add(Dense(100, activation='tanh',kernel_regularizer=regularizers.l2(1e-5)))\n",
    "classifier_BILSTM.add(Dropout(0.5))\n",
    "classifier_BILSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "classifier_BILSTM.compile(loss='binary_crossentropy',\n",
    "                       optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "classifier_BILSTM.fit(train_arrays, train_labels, batch_size=100, epochs=100, validation_data=(test_arrays,test_labels), shuffle=True)\n",
    "score = classifier_RNN.evaluate(test_arrays, test_labels)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
